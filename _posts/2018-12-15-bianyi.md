---
layout: post
title:  "编译程序回顾：以VASP-5.4.1，OCTOPUS-7.1为例"
date:   2018-12-15 13:43:00 +0800
categories: DFT
tags:  DFT Linux octopus
author: cndaqiang
mathjax: true
---
* content
{:toc}

回顾以前的编译坑






## [不保证正确]一些经验之谈
- 在不出现冲突的情况下:**并行编译器的使用**并不依赖于之前**编译并行编译器的串行编译器**<br>
**即:使用gcc-4.8.5编译的MVAPICH可以调用gcc-7.5进行并行代码编译**<br>
**mpif90实际上是一个脚本,里面定义了如何调用各种环境**<br>
- **编译的过程如果出现冲突,就不可以了**,不冲突的还可以执行
```
#mvapich依赖的libgfortran.so.3(gcc-4.8.5)与libgfortran.so.5(gcc-8)可不行
/usr/bin/ld: warning: libgfortran.so.3, needed by /public/software/mpi/mvapich2/gnu/2.3.1/lib/libmpifort.so, may conflict with libgfortran.so.5
#mvapich依赖的libgfortran.so.3(gcc-4.8.5)与libgfortran.so.4(gcc-7)可不行
/usr/bin/ld: warning: libgfortran.so.3, needed by /public/software/mpi/mvapich2/gnu/2.3.1/lib/libmpifort.so, may conflict with libgfortran.so.4
## 下面这些关系可能有助于判断
#### gcc 4.8.4/4.8.5提供(sslab)
libgfortran.so.3 ...
#### gcc-5.5.0(huairou)
libgfortran.so.3 ...
#### gcc7.5会编译(cnq)
libgfortran.so.4  libgfortran.so.4.0.0 libgfortran.a libgfortran.la ...
#### gcc 8/8.3 提供(sslab/huairou)
libgfortran.so.5
#### gcc 10.2.0(huairou)
libgfortran.so.5
```
- **但是使用gfortran&openmpi编译的用impi调用或者用openmpi调用ifort&impi编译的程序,会变成多个串行**
- 程序执行时需要的动态库会从LD_LIBRARY_PAT、系统缓存(`ldconfig -p` 可查)检索<br>
**编译器删除后,只要动态库备份了,就可以正常运行**,使用ldd查看程序运行需要的库
```
(python37) [SSLAB cndaqiang@login2 lib]$ls ~/libtest/
libblas.so  libgfortran.so.4  liblapack.so
export LD_LIBRARY_PATH=$HOME/libtest:$LD_LIBRARY_PATH
(python37) [SSLAB cndaqiang@login3 MoS2_1.6eV_6.6-gccQ-mpiQ.sh_deletGCC_addLDD]$ldd /public/home/cndaqiang/soft/gcc-7.5.0-mvapich/q-e-qe-6.6/TDAPW-6.6/bin/tdpw.x | grep found
	liblapack.so => not found
	libblas.so => not found
	libgfortran.so.4 => not found
	libgfortran.so.4 => not found
	libgfortran.so.4 => not found
(python37) [SSLAB cndaqiang@login3 MoS2_1.6eV_6.6-gccQ-mpiQ.sh_deletGCC_addLDD]$export LD_LIBRARY_PATH=$HOME/libtest:$LD_LIBRARY_PATH
(python37) [SSLAB cndaqiang@login3 MoS2_1.6eV_6.6-gccQ-mpiQ.sh_deletGCC_addLDD]$ldd /public/home/cndaqiang/soft/gcc-7.5.0-mvapich/q-e-qe-6.6/TDAPW-6.6/bin/tdpw.x | grep found
```
- **在A服务器上使用自己编译的gcc7和mvapich编译的程序,再将依赖的动态库(B服务器上有相同的库,比如mvapich的库就不发送了)和编译后的程序直接发送到B服务器,使用B服务器的mvapich提供的mpirun是可以直接运行程序的**
- **静态库因为还要连接不能混用,已经把静态库编入可执行程序后,可执行程序是可以拷贝到别的服务器上运行的**
- **`ldd`**可以查看可执行程序和动态库的依赖


## 编译器
### intel系 CC=icc FC=ifort CXX=icpc
使用
```
source /opt/intel/compilers_and_libraries_2018.0.128/linux/bin/compilervars.sh intel64
source /opt/intel/compilers_and_libraries_2018.0.128/linux/bin/iccvars.sh intel64 
source /opt/intel/compilers_and_libraries_2018.0.128/linux/bin/ifortvars.sh intel64
```
### GCC系 CC=gcc FC=gfortran CXX=g++
使用
```
export PATH=$GCCDIR/bin:$PATH
export LD_LIBRARY_PATH=$GCCDIR/lib64:$LD_LIBRARY_PATH
```


## 并行编程接口
### MPI 超算常用
[wiki-MPI](https://zh.wikipedia.org/wiki/%E8%A8%8A%E6%81%AF%E5%82%B3%E9%81%9E%E4%BB%8B%E9%9D%A2)消息传递界面/接口（英语：Message Passing Interface，缩写MPI）是一个并行计算的应用程序接口（API），常在超级计算机、计算机簇等非共享内存环境程序设计
<br>运行方式
```
mpirun -np 核数 $EXEC
```
每种MPI程序(如OPENMPI，MPICH2)使用的串行(ifort或gcc)编译器编译，形成不同的并行编译器(openmpi+ifort或openmpi+gcc)

#### OPENMPI
[Open MPI:Open Source High Performance Computing](https://www.open-mpi.org/)
<br>使用编译器编译OPENMPI参考[gcc Openmpi 编译siesta](/2018/09/12/gun-openmpi-siesta/#%E7%BC%96%E8%AF%91openmpi),只需把`FC CC CXX`指定即可
<br>如使用intel系`FC=ifort CC=icc CXX=icpc`,测试使用intel18的编译器编译openmpi-2.1.5与openmpi-4.0.0通过，其他版本可能存在兼容性问题
<br>个人编译，环境变量

```
export PATH=$MPIDIR/bin:$PATH
export LD_LIBRARY_PATH=$MPIDIR/lib:$LD_LIBRARY_PATH
```
超算上一般都编译好了不同的MPI程序和编译器的组合，如在天河上导入不同编译器与OPENMPI的组合示例
```
[AAA@lon6%tianhe2-B ~]$  module load openmpi/1.10-intel
[AAA@lon6%tianhe2-B ~]$ mpicc --version
icc (ICC) 14.0.2 20140120
Copyright (C) 1985-2014 Intel Corporation.  All rights reserved.


[AAA@lon12%tianhe2-B ~]$ module load openmpi/1.8.3
[AAA@lon12%tianhe2-B ~]$ mpicc --version
gcc (GCC) 4.4.7 20120313 (Red Hat 4.4.7-4)
Copyright (C) 2010 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
```

#### IMPI(Intel MPI)
Intel MPI－Intel基于开放源代码的MPICH2与MVAPICH2研发成的MPI
<br>安装parallel_studio_xe_2018可以安装，一般目录在`/opt/intel/impi/`,包含IMPI+intel系与IMPI+gcc系
<br>环境变量

```
source /opt/intel/impi/5.0.2.044/bin64/mpivars.sh
source  /opt/intel/impi/2018.0.128/bin64/mpivars.sh

#intel系 mpiicc mpiifort mpiicpc
#gcc系 mpicc mpif90 mpicxx
```
天河调用IMPI,包含IMPI+intel系与IMPI+gcc系,命令同上
```
[AAA@lon6%tianhe2-B ~]$ module load MPI/Intel/IMPI/4.1.3.048
[AAA@lon6%tianhe2-B ~]$ mpiifort --version
ifort (IFORT) 14.0.2 20140120
Copyright (C) 1985-2014 Intel Corporation.  All rights reserved.
```

#### MPICH等，我没用过，略

### Pthreads 略
### OPENMP
**注意OPENMP不是OPENMPI，OPENMPI是基于MPI的一种**<br>
依据[wiki-OpenMP](https://zh.wikipedia.org/zh-hans/OpenMP)的说法，OpenMP的另一个缺点是不能在非共享内存系统（如计算机集群）上使用。在这样的系统上，MPI使用较多。
<br>然后天河上好像也没有OPENMP的module，先略了



## 库
库与编译该库的编译器一定要对应<br>
有些程序使用两种库都可以安装，但测试的时候一种经常报错，如octopus-7.1与mkl，在`make check`时各种FAIL，而octopus与从netlib下载编译的库就没有
### MKL
[⭐自助调用⭐](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/)<br>
安装parallel_studio_xe_2018可以安装，一般目录在`/opt/intel/compilers_and_libraries_2018.3.222/linux/mkl/lib/intel64`
环境变量设置
```
source  /opt/intel/compilers_and_libraries_2018.3.222/linux/mkl/bin/mklvars.sh intel64
```
天河上也有相应模块`intel-compilers/mkl-15`等<br>
MKL主要包含如下内容：
- 基本线性代数子系统库(BLAS)
- 离散基本线性代数库(Sparse BLAS)
- 线性代数库(LAPACK)
- 可扩展性线性代数库(ScaLAPACK)
- 离散求解程序(Sparse Solver routines)
- 向量数学库函数(Vector Mathematical Library functions)
- 向量统计库函数(Vector Statistical Library functions)
- 傅立叶变换程序(Fourier Transform functions (FFT))
- 集群版傅立叶变换程序(Cluster FFT)
- 区间求解程序(Interval Solver routines)
- 三角变换程序(Trigonometric Transform routines)
- 泊松、拉普拉斯和哈密顿求解程序(Poisson, Laplace, and Helmholtz Solver routines)
- 优化（信赖域）求解程序(Optimization (Trust-Region) Solver routines)

注意调用的时候
- 有的库适合intel系编译器，有的适合gcc系，如`libmkl_gf_lp64.a`与`libmkl_intel_lp64.a`
- 有的库适合IMPI有的适合OPENMPI，如`libmkl_blacs_openmpi_ilp64.a`与`libmkl_blacs_intelmpi_ilp64.a`
- ilp与lp的区别：ilp支持的数据更大，编vasp的时候lp就可以了，ilp未尝试，[详细区别](https://software.intel.com/en-us/mkl-macos-developer-guide-using-the-ilp64-interface-vs-lp64-interface)
- 最懒的调用方式[⭐自助调用⭐](https://software.intel.com/en-us/articles/intel-mkl-link-line-advisor/)

### NETLIB安装
包含blas,blacs,lapack,scalapack等<br>
安装示例[gcc Openmpi 编译siesta](/2018/09/12/gun-openmpi-siesta/#%E7%BC%96%E8%AF%91openmpi)<br>
环境变量
```
MATHDIR=/home/cndaqiang/soft/scalapack/lib
export LD_LIBRARY_PATH=$MATHDIR:$LD_LIBRARY_PATH
```
调用方式
```
MATHLIB = -L$MATHDIR  -lrefblas -ltmg -lreflapack  -lscalapack
```

### FFTW
编译参考[ centos6.5 gcc Openmpi 编译octopus-4.1.2 ](/2018/09/15/gun-openmpi-octopus-4.1.2/)<br>
建议使用intel系编译器调用使用intel编译器编译的fftw,gcc系同理<br>
调用
```
LIBS_FFTW = -L/home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4/lib -lfftw3_mpi -lfftw3
```
**注意**,`libfftw3_mpi.a`仅包含并行部分的fftw代码，还要引用`libfftw3.a`，此处致谢[Ionizing Radiation](https://github.com/IonizingRadiation)
<br>在使用gcc系+netlib系数学库+fftw编译vasp.5.4.1时，要这样设置
<br>在使用intel系或者gcc系+mkl数学库+fftw编译vasp.5.4.1时，貌似因为mkl里面包含了部分fftw的代码，仅调用`-lfftw3_mpi`即可

### GSL libXC等
参考[ centos6.5 gcc Openmpi 编译octopus-4.1.2 ](/2018/09/15/gun-openmpi-octopus-4.1.2/)


## Example0-Intel+IMPI+mkl+vasp.5.4.1
如[Intel Parallel Studio XE 编译VASP ](/2018/01/15/intel-mpi-vasp/)

## Example1-GCC-4.8.4+openmpi-1.10.3+mkl-intel18+vasp.5.4.1
### 源码修改
使用gcc系编译vasp.5.4.1,要修改部分vasp源码，方法参考[Compiling VASP With Gfortran](https://www.nsc.liu.se/~pla/blog/2013/05/14/vasp-gcc/)，再次致谢[Ionizing Radiation](https://github.com/IonizingRadiation)<br>
修改`us.F`，按照下面，把-开的那行头变成+后面的内容
```
@@ -1460,7 +1460,7 @@ END MODULE
	   USE asa
	   USE paw
	   USE constant
-      USE us
+      USE us, only : setdij_
	   IMPLICIT NONE
 
	   TYPE (type_info)   T_INFO
@@ -2693,7 +2693,7 @@ END MODULE
	   USE mgrid
	   USE lattice
	   USE wave
-      USE us
+      USE us, only: augmentation_charge
```
### 库
```
cp arch/makefile.include.linux_gfortran makefile.include
```
修改`makefiel.include`
```
MKLROOT    = /opt/intel/compilers_and_libraries/linux/mkl
FC         = mpif90  -m64 -I${MKLROOT}/include
FCL        = mpif90  -m64 -I${MKLROOT}/include

BLAS       =  ${MKLROOT}/lib/intel64/libmkl_scalapack_lp64.a -Wl,--start-group ${MKLROOT}/lib/intel64/libmkl_cdft_core.a ${MKLROOT}/lib/intel64/libmkl_gf_lp64.a ${MKLROOT}/lib/intel64/libmkl_sequential.a ${MKLROOT}/lib/intel64/libmkl_core.a ${MKLROOT}/lib/intel64/libmkl_blacs_openmpi_lp64.a -Wl,--end-group -lpthread -lm -ldl

OBJECTS    = fftmpiw.o fftmpi_map.o  fftw3d.o  fft3dlib.o \
             /home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4/lib/libfftw3_mpi.a
INCS       =-I/home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4/include
```

## Example2-GCC-4.8.4+openmpi-1.10.3+netlib数学库+vasp.5.4.1
### 源码修改
同上
### 库
```
cp arch/makefile.include.linux_gfortran makefile.include
```
修改`makefiel.include`,**注意fftw的阴影和上面的不一样**
```
FC         = mpif90
FCL        = mpif90

LIBDIR     = /home/cndaqiang/soft/scalapack/lib
BLAS       = -L$(LIBDIR) -lrefblas
LAPACK     = -L$(LIBDIR) -ltmg -lreflapack -llapack
BLACS      =
SCALAPACK  = -L$(LIBDIR) -lscalapack $(BLACS)

OBJECTS    = fftmpiw.o fftmpi_map.o  fftw3d.o  fft3dlib.o \
             /home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4/lib/libfftw3_mpi.a \
             /home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4/lib/libfftw3.a
INCS       =-I/home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4/include
```

## Example3-GCC-4.8.4+openmpi-1.10.3+mkl+octopus
环境变量
```
GCCDIR=/home/cndaqiang/soft/gcc-4.8.4

MPIDIR=/home/cndaqiang/soft/openmpi-1.10.3_gcc-4.8.4

FFTWDIR=/home/cndaqiang/soft/OPENMPI-GCC/LIB/fftw-3.3.4

GSDIR=/home/cndaqiang/soft/gsl-1.14

LIBXCDIR=/home/cndaqiang/soft/libxc-2.0.3

MKLROOT=/opt/intel/compilers_and_libraries_2018.3.222/linux/mkl
source $MKLROOT/bin/mklvars.sh intel64

export PATH=$GCCDIR/bin:$PATH
export LD_LIBRARY_PATH=$GCCDIR/lib64:$LD_LIBRARY_PATH

export PATH=$MPIDIR/bin:$PATH
export LD_LIBRARY_PATH=$MPIDIR/lib:$LD_LIBRARY_PATH

export PATH=$FFTWDIR/bin:$PATH
export LD_LIBRARY_PATH=$FFTWDIR/lib:$LD_LIBRARY_PATH

export PATH=$GSDIR/bin:$PATH
export LD_LIBRARY_PATH=$GSDIR/lib:$LD_LIBRARY_PATH

export PATH=$LIBXCDIR/bin:$PATH
export LD_LIBRARY_PATH=$LIBXCDIR/lib:$LD_LIBRARY_PATH
```
配置Makefile
```
./configure --prefix=/home/cndaqiang/soft/octopus-7.1-build \
 ./configure --prefix=$(pwd)/../../octopus-7.2  --with-blas="-lmkl_gf_lp64 -lmkl_sequential -lmkl_core -lmkl_blas95_lp64" --with-libxc-prefix=$LIBXCDIR   --with-gsl-prefix=$GSDIR  --with-fftw-prefix=$FFTWDIR --enable-mpi CC=mpicc FC=mpif90 CXX=mpicxx FCFLAGS="-O3 -ffree-line-length-none" CFLAGS=-O3    
 ```
## Example4-GCC-4.8.4+openmpi-1.10.3+netlib数学库+octopus
环境变量同上
<br>配置Makefile
```
 ./configure --prefix=$(pwd)/../octopus-7,1-build-scalapack  --with-blas="-L$MATHDIR  -lrefblas -ltmg -lreflapack  -lscalapack" --with-libxc-prefix=$LIBXCDIR   --with-gsl-prefix=$GSDIR  --with-fftw-prefix=$FFTWDIR --enable-mpi CC=mpicc FC=mpif90 CXX=mpicxx FCFLAGS="-O3 -ffree-line-length-none" CFLAGS=-O3    
```


## 还需要
还需要好好去学fortran<br>
比如fortran默认单行长度最多132个字符，但是有的程序单行代码很多,如octopus8，对于gcc可以对gfortran(mpif90)添加`-ffree-line-length-none`编译参数解决







------
本文首发于[我的博客@cndaqiang](https://cndaqiang.github.io/).<br>
允许注明来源转发.<br>
强烈谴责大专栏等肆意转发全网技术博客不注明来源,还请求打赏的无耻行为.
